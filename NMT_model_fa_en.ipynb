{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NMT_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrn79/NMT/blob/src/NMT_model_fa_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QrMMuSVJkkWu"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import pandas as pd \n",
        "from tensorflow.keras import optimizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown --id 1M3r4dASBa5fUGA_Zreg071HlcllbXxbp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjAD6Nt7k4Ma",
        "outputId": "60520d5b-4ea8-4de9-98cd-f9a50f6f13dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1M3r4dASBa5fUGA_Zreg071HlcllbXxbp\n",
            "To: /content/language_data.csv\n",
            "100% 55.4M/55.4M [00:00<00:00, 147MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/language_data.csv') as f:\n",
        "  language_data = f.read()"
      ],
      "metadata": {
        "id": "QDgUpWPTk6jS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language_data = pd.read_csv('language_data.csv')\n",
        "english_text = language_data['English'].values\n",
        "persian_text = language_data['Persian'].values\n",
        "english_text[0]=english_text[0].replace('\\ufeff','')"
      ],
      "metadata": {
        "id": "1ia_JxhLlAoM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_text = english_text[:200000]\n",
        "persian_text = persian_text[:200000]"
      ],
      "metadata": {
        "id": "FvfSyKOyJJBM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase and remove punctuation in sentences\n",
        "def clean_sentence(sentence):\n",
        "    # Add a space ' ' befor the ? word\n",
        "    sentence = sentence.replace('?', ' ?')\n",
        "    # Lower case the sentence\n",
        "    lower_case_sent = sentence.lower().strip()\n",
        "    # Strip punctuation\n",
        "    string_punctuation = string.punctuation  # !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
        "    string_punctuation = string_punctuation.replace('?','')\n",
        "    # Clean the sentence\n",
        "    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
        "    clean_sentence = '<start> ' + clean_sentence + ' <end>'\n",
        "    return clean_sentence"
      ],
      "metadata": {
        "id": "XxyZKBzOkz93"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sent = [clean_sentence(pair) for pair in english_text]\n",
        "persian_sent = [clean_sentence(pair) for pair in persian_text]"
      ],
      "metadata": {
        "id": "B0q4SVuPmGET"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(persian_sent)"
      ],
      "metadata": {
        "id": "GrYHKAKdaMMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a669bee1-7a76-482d-896a-0a2d95feff21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert sequences to tokenizers\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  \n",
        "  # Convert sequences into internal vocab\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  # Convert internal vocab to numbers\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  # Pad the tensors to assign equal length to all the sequences\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "eWsYB2YZKTkd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "def load_dataset(targ_lang, inp_lang):\n",
        "\n",
        "  # Tokenize the sequences\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "metadata": {
        "id": "qQG-puKqLIp5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(english_sent,persian_sent)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "metadata": {
        "id": "cn6V_WgXLT6j"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create training and validation sets using an 80/20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCASn363Lw0l",
        "outputId": "8b01a8ac-4b23-497e-e71d-7943e93fe8af"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160000 160000 40000 40000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the mapping b/w word index and language tokenizer\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "      \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaoFS-pHMplV",
        "outputId": "34e74b55-7731-4c13-ca2f-4b6a1555f352"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "254 ----> البته\n",
            "300 ----> ممکنه\n",
            "12492 ----> شانسش\n",
            "292 ----> کم\n",
            "29 ----> باشه\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "97 ----> man\n",
            "632 ----> 2\n",
            "271 ----> hold\n",
            "47 ----> him\n",
            "6 ----> to\n",
            "8 ----> it\n",
            "55 ----> at\n",
            "4 ----> the\n",
            "320 ----> end\n",
            "10 ----> of\n",
            "4 ----> the\n",
            "3365 ----> season\n",
            "2 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential model parameters\n",
        "# Total number of input/target samples. In our model\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 150\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1"
      ],
      "metadata": {
        "id": "zw8lFvmLMzZ_"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "MASIM_MEM3vj"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of input and target batches\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJa6kHC4M_Bi",
        "outputId": "fd23cde5-1b49-4bba-de6f-d31d1a5909ad"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([256, 32]), TensorShape([256, 36]))"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder class\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "\n",
        "    # Embed the vocab to a dense embedding \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # GRU Layer\n",
        "    # glorot_uniform: Initializer for the recurrent_kernel weights matrix, \n",
        "    # used for the linear transformation of the recurrent state\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  # Encoder network comprises an Embedding layer followed by a GRU layer\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "\n",
        "  # To initialize the hidden state\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "metadata": {
        "id": "4lieNvBCNE2j"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIPxr00bNIvt",
        "outputId": "5267938d-49f4-4ca2-9572-8351479678f6"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (256, 32, 256)\n",
            "Encoder Hidden state shape: (batch size, units) (256, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Mechanism\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "vJ0YNZQBNPwb"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3miAlwSNU6e",
        "outputId": "2bc5b3c8-59c0-43f9-9cff-1ebf5e6219a1"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (256, 256)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (256, 32, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder class\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # x shape == (batch_size, 1)\n",
        "    # hidden shape == (batch_size, max_length)\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "    # context_vector shape == (batch_size, hidden_size)\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "metadata": {
        "id": "fnYFADvpNZy1"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX4aEszJNdl3",
        "outputId": "0e55dafe-819b-4a9a-fdce-8180634b8886"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (256, 37354)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize optimizer and loss functions\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# Loss function\n",
        "def loss_function(real, pred):\n",
        "\n",
        "  # Take care of the padding. Not all sequences are of equal length.\n",
        "  # If there's a '0' in the sequence, the loss is being nullified\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "iNXJlx2dNhiw"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "checkpoint_dir = './training_checkpoints_en'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "metadata": {
        "id": "tzpHf-3INkn9"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  # tf.GradientTape() -- record operations for automatic differentiation\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    # dec_hidden is used by attention, hence is the same enc_hidden\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # <start> token is the initial decoder input\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "\n",
        "      # Pass enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      # Compute the loss\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # Use teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  # As this function is called per batch, compute the batch_loss\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  # Get the model's variables\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  # Compute the gradients\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  # Update the variables of the model/network\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "metadata": {
        "id": "GlEXGBNXNpoj"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "EPOCHS = 25\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # Initialize the hidden state\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  # Loop through the dataset\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "\n",
        "    # Call the train method\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "\n",
        "    # Compute the loss (per batch)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # Save (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  # Output the loss observed until that epoch\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  \n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXKxCUzWNxVO",
        "outputId": "4167e19c-fcb9-4a88-a760-7146cd8b1268"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2802\n",
            "Epoch 1 Batch 100 Loss 1.3727\n",
            "Epoch 1 Batch 200 Loss 1.3913\n",
            "Epoch 1 Batch 300 Loss 1.3906\n",
            "Epoch 1 Batch 400 Loss 1.3052\n",
            "Epoch 1 Batch 500 Loss 1.3231\n",
            "Epoch 1 Batch 600 Loss 1.1987\n",
            "Epoch 1 Loss 1.3594\n",
            "Time taken for 1 epoch 244.67890739440918 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2960\n",
            "Epoch 2 Batch 100 Loss 1.2122\n",
            "Epoch 2 Batch 200 Loss 1.1812\n",
            "Epoch 2 Batch 300 Loss 1.1602\n",
            "Epoch 2 Batch 400 Loss 1.1868\n",
            "Epoch 2 Batch 500 Loss 1.1177\n",
            "Epoch 2 Batch 600 Loss 1.1450\n",
            "Epoch 2 Loss 1.1900\n",
            "Time taken for 1 epoch 203.76611733436584 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.0904\n",
            "Epoch 3 Batch 100 Loss 1.1480\n",
            "Epoch 3 Batch 200 Loss 1.0419\n",
            "Epoch 3 Batch 300 Loss 1.1024\n",
            "Epoch 3 Batch 400 Loss 1.0702\n",
            "Epoch 3 Batch 500 Loss 1.0262\n",
            "Epoch 3 Batch 600 Loss 1.0103\n",
            "Epoch 3 Loss 1.0852\n",
            "Time taken for 1 epoch 203.3844006061554 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.9987\n",
            "Epoch 4 Batch 100 Loss 1.0096\n",
            "Epoch 4 Batch 200 Loss 1.0160\n",
            "Epoch 4 Batch 300 Loss 0.9635\n",
            "Epoch 4 Batch 400 Loss 1.0201\n",
            "Epoch 4 Batch 500 Loss 0.9348\n",
            "Epoch 4 Batch 600 Loss 0.9020\n",
            "Epoch 4 Loss 0.9870\n",
            "Time taken for 1 epoch 204.00848722457886 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.9071\n",
            "Epoch 5 Batch 100 Loss 0.8931\n",
            "Epoch 5 Batch 200 Loss 0.8931\n",
            "Epoch 5 Batch 300 Loss 0.8829\n",
            "Epoch 5 Batch 400 Loss 0.8844\n",
            "Epoch 5 Batch 500 Loss 0.9067\n",
            "Epoch 5 Batch 600 Loss 0.8732\n",
            "Epoch 5 Loss 0.8976\n",
            "Time taken for 1 epoch 203.57192468643188 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.8575\n",
            "Epoch 6 Batch 100 Loss 0.8097\n",
            "Epoch 6 Batch 200 Loss 0.8351\n",
            "Epoch 6 Batch 300 Loss 0.8314\n",
            "Epoch 6 Batch 400 Loss 0.8127\n",
            "Epoch 6 Batch 500 Loss 0.8412\n",
            "Epoch 6 Batch 600 Loss 0.8318\n",
            "Epoch 6 Loss 0.8206\n",
            "Time taken for 1 epoch 204.19512844085693 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.7425\n",
            "Epoch 7 Batch 100 Loss 0.7885\n",
            "Epoch 7 Batch 200 Loss 0.7670\n",
            "Epoch 7 Batch 300 Loss 0.7573\n",
            "Epoch 7 Batch 400 Loss 0.7390\n",
            "Epoch 7 Batch 500 Loss 0.7504\n",
            "Epoch 7 Batch 600 Loss 0.7778\n",
            "Epoch 7 Loss 0.7532\n",
            "Time taken for 1 epoch 203.76607656478882 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.6979\n",
            "Epoch 8 Batch 100 Loss 0.6642\n",
            "Epoch 8 Batch 200 Loss 0.6731\n",
            "Epoch 8 Batch 300 Loss 0.6426\n",
            "Epoch 8 Batch 400 Loss 0.6897\n",
            "Epoch 8 Batch 500 Loss 0.6702\n",
            "Epoch 8 Batch 600 Loss 0.7407\n",
            "Epoch 8 Loss 0.6938\n",
            "Time taken for 1 epoch 204.09865260124207 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.5915\n",
            "Epoch 9 Batch 100 Loss 0.6321\n",
            "Epoch 9 Batch 200 Loss 0.6497\n",
            "Epoch 9 Batch 300 Loss 0.7082\n",
            "Epoch 9 Batch 400 Loss 0.7019\n",
            "Epoch 9 Batch 500 Loss 0.6468\n",
            "Epoch 9 Batch 600 Loss 0.6302\n",
            "Epoch 9 Loss 0.6417\n",
            "Time taken for 1 epoch 203.61423063278198 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.6059\n",
            "Epoch 10 Batch 100 Loss 0.6104\n",
            "Epoch 10 Batch 200 Loss 0.5714\n",
            "Epoch 10 Batch 300 Loss 0.5856\n",
            "Epoch 10 Batch 400 Loss 0.5835\n",
            "Epoch 10 Batch 500 Loss 0.6282\n",
            "Epoch 10 Batch 600 Loss 0.5806\n",
            "Epoch 10 Loss 0.5954\n",
            "Time taken for 1 epoch 204.25752592086792 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.5379\n",
            "Epoch 11 Batch 100 Loss 0.5453\n",
            "Epoch 11 Batch 200 Loss 0.5360\n",
            "Epoch 11 Batch 300 Loss 0.5947\n",
            "Epoch 11 Batch 400 Loss 0.4941\n",
            "Epoch 11 Batch 500 Loss 0.5940\n",
            "Epoch 11 Batch 600 Loss 0.5765\n",
            "Epoch 11 Loss 0.5541\n",
            "Time taken for 1 epoch 203.72202920913696 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.4980\n",
            "Epoch 12 Batch 100 Loss 0.4802\n",
            "Epoch 12 Batch 200 Loss 0.4730\n",
            "Epoch 12 Batch 300 Loss 0.4987\n",
            "Epoch 12 Batch 400 Loss 0.5116\n",
            "Epoch 12 Batch 500 Loss 0.5046\n",
            "Epoch 12 Batch 600 Loss 0.5151\n",
            "Epoch 12 Loss 0.5189\n",
            "Time taken for 1 epoch 204.1970911026001 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.4885\n",
            "Epoch 13 Batch 100 Loss 0.4702\n",
            "Epoch 13 Batch 200 Loss 0.4714\n",
            "Epoch 13 Batch 300 Loss 0.4675\n",
            "Epoch 13 Batch 400 Loss 0.5052\n",
            "Epoch 13 Batch 500 Loss 0.4933\n",
            "Epoch 13 Batch 600 Loss 0.5067\n",
            "Epoch 13 Loss 0.4856\n",
            "Time taken for 1 epoch 203.69933438301086 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.4441\n",
            "Epoch 14 Batch 100 Loss 0.4440\n",
            "Epoch 14 Batch 200 Loss 0.4342\n",
            "Epoch 14 Batch 300 Loss 0.4910\n",
            "Epoch 14 Batch 400 Loss 0.4620\n",
            "Epoch 14 Batch 500 Loss 0.4413\n",
            "Epoch 14 Batch 600 Loss 0.4981\n",
            "Epoch 14 Loss 0.4564\n",
            "Time taken for 1 epoch 204.31315398216248 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.4524\n",
            "Epoch 15 Batch 100 Loss 0.4084\n",
            "Epoch 15 Batch 200 Loss 0.4199\n",
            "Epoch 15 Batch 300 Loss 0.4290\n",
            "Epoch 15 Batch 400 Loss 0.4242\n",
            "Epoch 15 Batch 500 Loss 0.4599\n",
            "Epoch 15 Batch 600 Loss 0.4408\n",
            "Epoch 15 Loss 0.4313\n",
            "Time taken for 1 epoch 203.61779117584229 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.3848\n",
            "Epoch 16 Batch 100 Loss 0.4083\n",
            "Epoch 16 Batch 200 Loss 0.4277\n",
            "Epoch 16 Batch 300 Loss 0.4566\n",
            "Epoch 16 Batch 400 Loss 0.3829\n",
            "Epoch 16 Batch 500 Loss 0.4146\n",
            "Epoch 16 Batch 600 Loss 0.4384\n",
            "Epoch 16 Loss 0.4087\n",
            "Time taken for 1 epoch 204.1492040157318 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.4191\n",
            "Epoch 17 Batch 100 Loss 0.3459\n",
            "Epoch 17 Batch 200 Loss 0.4051\n",
            "Epoch 17 Batch 300 Loss 0.3490\n",
            "Epoch 17 Batch 400 Loss 0.4124\n",
            "Epoch 17 Batch 500 Loss 0.3747\n",
            "Epoch 17 Batch 600 Loss 0.4253\n",
            "Epoch 17 Loss 0.3867\n",
            "Time taken for 1 epoch 203.73986649513245 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.3404\n",
            "Epoch 18 Batch 100 Loss 0.3429\n",
            "Epoch 18 Batch 200 Loss 0.3845\n",
            "Epoch 18 Batch 300 Loss 0.3763\n",
            "Epoch 18 Batch 400 Loss 0.3767\n",
            "Epoch 18 Batch 500 Loss 0.3534\n",
            "Epoch 18 Batch 600 Loss 0.4094\n",
            "Epoch 18 Loss 0.3672\n",
            "Time taken for 1 epoch 204.3754231929779 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.3026\n",
            "Epoch 19 Batch 100 Loss 0.3254\n",
            "Epoch 19 Batch 200 Loss 0.3461\n",
            "Epoch 19 Batch 300 Loss 0.3722\n",
            "Epoch 19 Batch 400 Loss 0.3462\n",
            "Epoch 19 Batch 500 Loss 0.3501\n",
            "Epoch 19 Batch 600 Loss 0.3816\n",
            "Epoch 19 Loss 0.3504\n",
            "Time taken for 1 epoch 203.56378412246704 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.3085\n",
            "Epoch 20 Batch 100 Loss 0.3032\n",
            "Epoch 20 Batch 200 Loss 0.3188\n",
            "Epoch 20 Batch 300 Loss 0.3358\n",
            "Epoch 20 Batch 400 Loss 0.3585\n",
            "Epoch 20 Batch 500 Loss 0.3239\n",
            "Epoch 20 Batch 600 Loss 0.3502\n",
            "Epoch 20 Loss 0.3341\n",
            "Time taken for 1 epoch 204.17579579353333 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.2972\n",
            "Epoch 21 Batch 100 Loss 0.3200\n",
            "Epoch 21 Batch 200 Loss 0.2928\n",
            "Epoch 21 Batch 300 Loss 0.3369\n",
            "Epoch 21 Batch 400 Loss 0.3185\n",
            "Epoch 21 Batch 500 Loss 0.2971\n",
            "Epoch 21 Batch 600 Loss 0.3377\n",
            "Epoch 21 Loss 0.3182\n",
            "Time taken for 1 epoch 203.61409997940063 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.2849\n",
            "Epoch 22 Batch 100 Loss 0.3122\n",
            "Epoch 22 Batch 200 Loss 0.2984\n",
            "Epoch 22 Batch 300 Loss 0.2920\n",
            "Epoch 22 Batch 400 Loss 0.2690\n",
            "Epoch 22 Batch 500 Loss 0.3207\n",
            "Epoch 22 Batch 600 Loss 0.3117\n",
            "Epoch 22 Loss 0.3041\n",
            "Time taken for 1 epoch 204.20730257034302 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.2553\n",
            "Epoch 23 Batch 100 Loss 0.2910\n",
            "Epoch 23 Batch 200 Loss 0.2884\n",
            "Epoch 23 Batch 300 Loss 0.2891\n",
            "Epoch 23 Batch 400 Loss 0.3253\n",
            "Epoch 23 Batch 500 Loss 0.3161\n",
            "Epoch 23 Batch 600 Loss 0.3223\n",
            "Epoch 23 Loss 0.2903\n",
            "Time taken for 1 epoch 203.6831660270691 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.2785\n",
            "Epoch 24 Batch 100 Loss 0.2598\n",
            "Epoch 24 Batch 200 Loss 0.2739\n",
            "Epoch 24 Batch 300 Loss 0.2862\n",
            "Epoch 24 Batch 400 Loss 0.2583\n",
            "Epoch 24 Batch 500 Loss 0.3025\n",
            "Epoch 24 Batch 600 Loss 0.3262\n",
            "Epoch 24 Loss 0.2789\n",
            "Time taken for 1 epoch 204.42795038223267 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.2169\n",
            "Epoch 25 Batch 100 Loss 0.2543\n",
            "Epoch 25 Batch 200 Loss 0.2381\n",
            "Epoch 25 Batch 300 Loss 0.2893\n",
            "Epoch 25 Batch 400 Loss 0.2610\n",
            "Epoch 25 Batch 500 Loss 0.2811\n",
            "Epoch 25 Batch 600 Loss 0.2783\n",
            "Epoch 25 Loss 0.2686\n",
            "Time taken for 1 epoch 203.59091758728027 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Evaluate function -- similar to the training loop\n",
        "def evaluate(sentence):\n",
        "\n",
        "  # Attention plot (to be plotted later on) -- initialized with max_lengths of both target and input\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  # Preprocess the sentence given\n",
        "  sentence = clean_sentence(sentence)\n",
        "\n",
        "  # Fetch the indices concerning the words in the sentence and pad the sequence\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  # Convert the inputs to tensors\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  # Loop until the max_length is reached for the target lang (ENGLISH)\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # Store the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    # Get the prediction with the maximum attention\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    # Append the token to the result\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    # If <end> token is reached, return the result, input, and attention plot\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # The predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "metadata": {
        "id": "6uyc0b_bmR6S"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ZiB6r2mkm8jm"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate function (which internally calls the evaluate function)\n",
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        " # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "metadata": {
        "id": "HW11GJXtnHpz"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1siVzlEsnOG0",
        "outputId": "6680ab73-f5fd-4ba5-a885-e58094ad5fc3"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6a1815bd50>"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(u\"سلام چه طوری\")\n",
        "translate(u\"بیا باهم صحبت کنیم\")\n",
        "translate(u\"اهل کجایی هستی دوست من\")\n",
        "translate(u\"اسمت چیه\")\n",
        "translate(u\"بریم خرید\")\n",
        "translate(u\"به مدرسه می روم\")\n",
        "translate(u\"شغلت چیه\")\n",
        "translate(u\"پدرت کجاست\")\n",
        "translate(u\"خانه شما کجاست\")\n",
        "translate(u\"خداحافظ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1x4xtBEnWGb",
        "outputId": "174977d9-afb4-49ed-9d47-2dc0e0324f04"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: <start> سلام چه طوری <end>\n",
            "Predicted translation: hello what <end> \n",
            "Input: <start> بیا باهم صحبت کنیم <end>\n",
            "Predicted translation: lets talk together <end> \n",
            "Input: <start> اهل کجایی هستی دوست من <end>\n",
            "Predicted translation: where are you from my friend <end> \n",
            "Input: <start> اسمت چیه <end>\n",
            "Predicted translation: whats your name <end> \n",
            "Input: <start> بریم خرید <end>\n",
            "Predicted translation: lets go <end> \n",
            "Input: <start> به مدرسه می روم <end>\n",
            "Predicted translation: at school <end> \n",
            "Input: <start> شغلت چیه <end>\n",
            "Predicted translation: what is that <end> \n",
            "Input: <start> پدرت کجاست <end>\n",
            "Predicted translation: where your father <end> \n",
            "Input: <start> خانه شما کجاست <end>\n",
            "Predicted translation: where is your house <end> \n",
            "Input: <start> خداحافظ <end>\n",
            "Predicted translation: bye <end> \n"
          ]
        }
      ]
    }
  ]
}